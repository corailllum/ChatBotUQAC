"""
Chatbot RAG - Version corrigÃ©e
Posez vos questions sur le Manuel de Gestion UQAC
"""

from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM, OllamaEmbeddings
import streamlit as st
import config

# ========================
# 1. INTERFACE
# ========================
st.set_page_config(page_title="Chatbot RAG UQAC", page_icon="ðŸ’¬", layout="wide")
st.title("ðŸ’¬ Chatbot RAG - Manuel de Gestion UQAC")
st.caption("Posez vos questions sur les politiques et rÃ¨glements de l'UQAC")

# Sidebar pour les paramÃ¨tres
with st.sidebar:
    st.header("âš™ï¸ ParamÃ¨tres")
    k_sources = st.slider(
        "Nombre de sources Ã  consulter",
        min_value=1,
        max_value=10,
        value=4,
        help="Plus de sources = plus de contexte mais temps de rÃ©ponse plus long"
    )

    if st.button("ðŸ—‘ï¸ Effacer l'historique"):
        st.session_state.messages = []
        st.session_state.conversation_context = []
        st.rerun()

# ========================
# 2. INITIALISATION
# ========================
@st.cache_resource
def init_components():
    """Initialise les composants (embeddings, vectorstore, LLM)"""
    embeddings = OllamaEmbeddings(model=config.EMBEDDING_MODEL)
    vectorstore = Chroma(
        persist_directory=config.PERSIST_DIRECTORY,
        embedding_function=embeddings
    )
    llm = OllamaLLM(model=config.LLM_MODEL, temperature=0.2)
    return embeddings, vectorstore, llm


embeddings, vectorstore, llm = init_components()

# ========================
# 3. FONCTION RAG AVEC MÃ‰MOIRE
# ========================
def get_rag_response(question: str, k: int = 4):
    """
    GÃ©nÃ¨re une rÃ©ponse en utilisant RAG avec mÃ©moire contextuelle optionnelle

    Args:
        question: La question de l'utilisateur
        k: Nombre de documents sources Ã  rÃ©cupÃ©rer

    Returns:
       Dictionnaire avec le contexte et les sources
    """

    # RÃ©cupÃ©rer les documents pertinents
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    source_docs = retriever.invoke(question)

    # Construire le contexte depuis les sources
    context = "\n\n".join([
        f"[Source {i + 1}]\n{doc.page_content}"
        for i, doc in enumerate(source_docs)
    ])

    # Construire l'historique de conversation
    conversation_history = ""
    if "conversation_context" in st.session_state:
        recent_exchanges = st.session_state.conversation_context[-3:]  # 3 derniers Ã©changes
        if recent_exchanges:
            conversation_history = "\n\nHistorique rÃ©cent de la conversation:\n"
            for exchange in recent_exchanges:
                conversation_history += f"Q: {exchange['question']}\n"
                conversation_history += f"R: {exchange['answer']}\n\n"

    # CrÃ©er le prompt avec ou sans mÃ©moire
    if conversation_history:
        template = """Tu es un assistant spÃ©cialisÃ© dans les politiques et procÃ©dures de l'UQAC.
                    RÃ©ponds en te basant sur le contexte fourni et l'historique de la conversation.
                    Si l'information n'est pas dans le contexte, dis-le clairement.
                    
                    {conversation_history}
                    
                    Contexte actuel:
                    {context}
                    
                    Question actuelle: {question}
                    
                    RÃ©ponse:
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain_input = {
            "context": context,
            "question": question,
            "conversation_history": conversation_history
        }
    else:
        template = """Tu es un assistant spÃ©cialisÃ© dans les politiques de l'UQAC.
                    RÃ©ponds en te basant uniquement sur le contexte fourni.
                    Si l'information n'est pas dans le contexte, dis-le clairement.
                    
                    Contexte:
                    {context}
                    
                    Question: {question}
                    
                    RÃ©ponse:
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain_input = {
            "context": context,
            "question": question
        }

    # GÃ©nÃ©rer la rÃ©ponse
    formatted_prompt = prompt.format(**chain_input)
    answer = llm.invoke(formatted_prompt)

    return {
        "answer": answer,
        "sources": source_docs
    }

# ========================
# 4. INITIALISATION DE LA SESSION
# ========================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "conversation_context" not in st.session_state:
    st.session_state.conversation_context = []

# ========================
# 5. AFFICHAGE DE L'HISTORIQUE
# ========================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # Afficher les sources si disponibles
        if message["role"] == "assistant" and "sources" in message:
            with st.expander(f"ðŸ“š {len(message['sources'])} sources consultÃ©es"):
                for i, doc in enumerate(message["sources"], 1):
                    url = doc.metadata.get('url', 'N/A')

                    st.markdown(f"{i}. {url}")

                    # Afficher un extrait du contenu
                    preview = doc.page_content[:200].replace('\n', ' ')
                    st.text(f"   {preview}...")
                    st.divider()

# ========================
# 6. ENTRÃ‰E UTILISATEUR
# ========================
if prompt := st.chat_input("Votre question sur le manuel de gestion..."):

    # Afficher la question de l'utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # GÃ©nÃ©rer et afficher la rÃ©ponse
    with st.chat_message("assistant"):
        with st.spinner("ðŸ” Recherche dans le manuel de gestion..."):
            result = get_rag_response(prompt, k=k_sources)
            answer = result["answer"]
            sources = result["sources"]

            # Afficher la rÃ©ponse
            st.markdown(answer)

            # Afficher les sources de maniÃ¨re persistante
            with st.expander(f"ðŸ“š {len(sources)} sources consultÃ©es"):
                for i, doc in enumerate(sources, 1):
                    url = doc.metadata.get('url', 'N/A')

                    st.markdown(f"{i}. {url}")

                    # Afficher un extrait du contenu
                    preview = doc.page_content[:200].replace('\n', ' ')
                    st.text(f"   {preview}...")
                    st.divider()

    # Sauvegarder la rÃ©ponse avec les sources dans l'historique
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer,
        "sources": sources
    })

    # Mettre Ã  jour le contexte de conversation pour la mÃ©moire
    st.session_state.conversation_context.append({
        "question": prompt,
        "answer": answer
    })

    # Limiter l'historique contextuel Ã  5 Ã©changes pour Ã©viter des prompts trop longs
    if len(st.session_state.conversation_context) > 5:
        st.session_state.conversation_context = st.session_state.conversation_context[-5:]

# ========================
# 7. FOOTER AVEC INFOS
# ========================
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("ðŸ¤– ModÃ¨le LLM", config.LLM_MODEL)
with col2:
    st.metric("ðŸ§  ModÃ¨le Embeddings", config.EMBEDDING_MODEL)
with col3:
    st.metric("ðŸ’¬ Messages", len(st.session_state.messages))